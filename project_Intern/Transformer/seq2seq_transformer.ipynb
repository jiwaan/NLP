{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchtext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We use the English-French translation data downloaded last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_en = pd.read_pickle('../wk14/data/cleaned_en.pkl')\n",
    "cleaned_fr = pd.read_pickle('../wk14/data/cleaned_fr.pkl')\n",
    "\n",
    "en_fr = pd.DataFrame({'en': cleaned_en, 'fr': cleaned_fr})\n",
    "en_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((en_fr['en'].apply(len) < 1) | (en_fr['fr'].apply(len) < 1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr.drop(en_fr[(en_fr['en'].apply(len) < 1) | (en_fr['fr'].apply(len) < 1)].index, inplace=True)\n",
    "((en_fr['en'].apply(len) < 1) | (en_fr['fr'].apply(len) < 1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_valid_test_split(df, train_size=0.8, valid_size=0.1, test_size=0.1, random_state=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Split the dataset into train, valid, and test set.\n",
    "    \"\"\"\n",
    "    if shuffle is True:\n",
    "        df = df.sample(frac=1, random_state=random_state)\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=valid_size/(train_size+valid_size), random_state=random_state)\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "def build_vocab(df, tokenizer, specials=['<unk>', '<pad>', '<bos>', '<eos>'], min_freq=2):\n",
    "    \"\"\"\n",
    "    Build vocabulary from the dataset.\n",
    "    :param specials:\n",
    "        - <unk>: unknown token\n",
    "        - <pad>: padding token\n",
    "        - <bos>: beginning of sentence token\n",
    "        - <eos>: end of sentence token\n",
    "    :param min_freq: minimum frequency of the token to be included in the vocabulary.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, df), specials=specials, min_freq=min_freq)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def save_data(data, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        print(f'Data saved to {path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data sampling\n",
    "\n",
    "It is difficult to use all of this data due to the memory capacity issues.\n",
    "Therefore, we will sample sentences that are not too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_num_words = en_fr['en'].apply(lambda x: len(x.split()))\n",
    "fr_num_words = en_fr['fr'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect the distribution of the number of words in the sentences\n",
    "print(en_num_words.describe())\n",
    "en_num_words.plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fr_num_words.describe())\n",
    "fr_num_words.plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_num_words[en_num_words < 10].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_num_words[(en_num_words > 65) & (en_num_words < 75)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr[en_num_words == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_fr = en_fr.sample(frac=0.5, random_state=42)\n",
    "en_fr = en_fr[(en_num_words <= 72) & (fr_num_words <= 72)]\n",
    "en_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((en_fr['en'].apply(len) < 1) & (en_fr['fr'].apply(len) < 1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = train_valid_test_split(en_fr, train_size=0.8, valid_size=0.1, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_vocab = build_vocab(train_df['en'], en_tokenizer, min_freq=5)\n",
    "fr_vocab = build_vocab(train_df['fr'], fr_tokenizer, min_freq=5)\n",
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_data((train_df, valid_df, test_df), 'data/train_valid_test.pkl')\n",
    "# save_data((en_vocab, fr_vocab), 'data/vocab.pkl')\n",
    "\n",
    "save_data((train_df, valid_df, test_df), 'data/small_train_valid_test.pkl')\n",
    "save_data((en_vocab, fr_vocab), 'data/small_vocab.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, string\n",
    "from unicodedata import normalize\n",
    "\n",
    "from typing import Optional, List, Tuple, Dict, Iterable, Callable\n",
    "\n",
    "def clean_lines(lines):\n",
    "    if isinstance(lines, list):\n",
    "        return [clean_lines(line) for line in lines]\n",
    "\n",
    "    is_question = lines.endswith('?')\n",
    "    remove_punctuation = str.maketrans('', '', string.punctuation)\n",
    "    lines = normalize('NFD', lines).encode('ascii', 'ignore')\n",
    "    lines = lines.decode('UTF-8')\n",
    "    lines = lines.lower()\n",
    "    lines = lines.translate(remove_punctuation)\n",
    "    lines = re.sub(rf'[^{re.escape(string.printable)}]', '', lines)\n",
    "\n",
    "    lines = [word for word in lines.split() if word.isalpha()]\n",
    "    if is_question:\n",
    "        lines.append('?')\n",
    "    return ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "train_df, valid_df, test_df = pd.read_pickle('data/small_train_valid_test.pkl')\n",
    "en_vocab, fr_vocab = pd.read_pickle('data/small_vocab.pkl')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SRC_LANG = 'fr'\n",
    "TGT_LANG = 'en'\n",
    "\n",
    "vocab_transform = {\n",
    "    # List[str] -> List[int]\n",
    "    'fr': fr_vocab,\n",
    "    'en': en_vocab\n",
    "}\n",
    "\n",
    "tokenizer_transform = {\n",
    "    # str -> List[str]\n",
    "    'fr': fr_tokenizer,\n",
    "    'en': en_tokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BOS_IDX = vocab_transform[TGT_LANG]['<bos>']\n",
    "EOS_IDX = vocab_transform[TGT_LANG]['<eos>']\n",
    "PAD_IDX = vocab_transform[TGT_LANG]['<pad>']\n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    # Compose several transforms to be applied sequentially.\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def append_special(token_ids: List[int]):\n",
    "    return th.cat([\n",
    "        th.tensor([BOS_IDX]),\n",
    "        th.tensor(token_ids),\n",
    "        th.tensor([EOS_IDX])\n",
    "    ])\n",
    "\n",
    "text_transform = {lang: sequential_transforms(tokenizer_transform[lang],\n",
    "                                               vocab_transform[lang],\n",
    "                                               append_special)\n",
    "                    for lang in [SRC_LANG, TGT_LANG]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function defines how to process a batch of data into a batch of tensors.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANG](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANG](tgt_sample.rstrip(\"\\n\")))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_iter = DataLoader(list(zip(train_df[SRC_LANG], train_df[TGT_LANG])),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_iter = DataLoader(list(zip(valid_df[SRC_LANG], valid_df[TGT_LANG])),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_iter = DataLoader(list(zip(test_df[SRC_LANG], test_df[TGT_LANG])),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_data, tgt_data = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_data.shape, tgt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join([vocab_transform[SRC_LANG].get_itos()[i] for i in src_data[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join([vocab_transform[TGT_LANG].get_itos()[i] for i in tgt_data[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_dim: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_len: int = 5000,\n",
    "                 batch_first: bool = False):\n",
    "\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        density = th.exp(-th.arange(0, emb_dim, 2) * np.log(10000) / emb_dim)\n",
    "        pos = th.arange(0, max_len).unsqueeze(1)\n",
    "        pos_embedding = th.zeros((max_len, emb_dim))\n",
    "        pos_embedding[:, 0::2] = th.sin(pos * density)\n",
    "        pos_embedding[:, 1::2] = th.cos(pos * density)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)  # [max_len, 1, emb_dim]\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [seq_len, batch_size, emb_dim] or [batch_size, seq_len, emb_dim]\n",
    "        if self.batch_first:\n",
    "            return self.dropout(x + self.pos_embedding[:x.size(1), :].permute(1, 0, 2))\n",
    "        else:\n",
    "            return self.dropout(x + self.pos_embedding[:x.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [seq_len, batch_size]\n",
    "        return self.emb(x.long()) * np.sqrt(self.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [seq_len, batch_size]\n",
    "        return self.emb(x.long()) * np.sqrt(self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = PositionalEncoding(512, dropout=0.1, max_len=48, batch_first=False)\n",
    "positional_vector = pos.pos_embedding\n",
    "positional_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.pcolormesh(positional_vector[:, 0], cmap='RdBu')\n",
    "plt.xlim(0, 512)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_enc_layers: int,\n",
    "                 num_dec_layers: int,\n",
    "                 emb_dim: int,\n",
    "                 n_heads: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_ff: int = 512,\n",
    "                 dropout: float = 0.1,\n",
    "                 batch_first: bool = False):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.src_emb = nn.Sequential(TokenEmbedding(src_vocab_size, emb_dim),\n",
    "                                     PositionalEncoding(emb_dim, dropout=dropout, batch_first=batch_first))\n",
    "        self.tgt_emb = nn.Sequential(TokenEmbedding(tgt_vocab_size, emb_dim),\n",
    "                                     PositionalEncoding(emb_dim, dropout=dropout, batch_first=batch_first))\n",
    "        self.transformer = nn.Transformer(d_model=emb_dim,\n",
    "                                          nhead=n_heads,\n",
    "                                          num_encoder_layers=num_enc_layers,\n",
    "                                          num_decoder_layers=num_dec_layers,\n",
    "                                          dim_feedforward=dim_ff,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=batch_first)\n",
    "        self.regressor = nn.Linear(emb_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src: th.Tensor, tgt: th.Tensor,\n",
    "                src_mask: th.Tensor = None, tgt_mask: th.Tensor = None,\n",
    "                src_padding_mask: th.Tensor = None,\n",
    "                tgt_padding_mask: th.Tensor = None,\n",
    "                memory_key_padding_mask: th.Tensor = None) -> th.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        :param src:         [seq_len, bs] or [bs, seq_len]\n",
    "        :param tgt:         [seq_len, bs] or [bs, seq_len]\n",
    "        :param src_mask:    [seq_len, bs, seq_len] or [bs, seq_len, seq_len]\n",
    "        :param tgt_mask:    [seq_len, bs, seq_len] or [bs, seq_len, seq_len]\n",
    "        :param src_padding_mask:    [seq_len, bs] or [bs, seq_len]\n",
    "        :param tgt_padding_mask:    [seq_len, bs] or [bs, seq_len]\n",
    "        :param memory_key_padding_mask: [seq_len, bs] or [bs, seq_len]\n",
    "        :return:    [seq_len, bs, vocab_size] or [bs, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # src = [seq_len, batch_size] or [batch_size, seq_len]\n",
    "        # tgt = [seq_len, batch_size] or [batch_size, seq_len]\n",
    "        src = self.src_emb(src)\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        # [seq_len, bs, emb_dim] or [bs, seq_len, emb_dim]\n",
    "        output = self.transformer(src, tgt,\n",
    "                                  src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "                                  src_key_padding_mask=src_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        output = self.regressor(output)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src: th.Tensor, src_mask: th.Tensor):\n",
    "        src = self.src_emb(src)\n",
    "        return self.transformer.encoder(src, src_mask=src_mask)\n",
    "\n",
    "    def decode(self, tgt: th.Tensor, memory: th.Tensor, tgt_mask: th.Tensor = None, memory_mask: th.Tensor=None):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        return self.transformer.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def look_ahead_mask(seq_len: int, device: th.device = th.device('cpu')):\n",
    "    # look ahead mask for decoder 1'st layer\n",
    "    mask = th.triu(th.ones((seq_len, seq_len), device=device)).transpose(0, 1)\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "    mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src: th.Tensor, tgt: th.Tensor, pad_idx: int = PAD_IDX, batch_first: bool = False):\n",
    "    \"\"\"\n",
    "    src_mask: all True mask, [seq_len, seq_len]\n",
    "    tgt_mask: look ahead mask, [seq_len, seq_len]\n",
    "    src_padding_mask: [seq_len, bs] or [bs, seq_len]\n",
    "    tgt_padding_mask: [seq_len, bs] or [bs, seq_len]\n",
    "    \"\"\"\n",
    "    src_seq_len = src.size(-1 if batch_first else 0)\n",
    "    tgt_seq_len = tgt.size(-1 if batch_first else 0)\n",
    "\n",
    "    tgt_mask = look_ahead_mask(tgt_seq_len, device=tgt.device)\n",
    "    src_mask = th.ones((src_seq_len, src_seq_len), device=src.device)\n",
    "\n",
    "    src_padding_mask = (src == pad_idx)\n",
    "    tgt_padding_mask = (tgt == pad_idx)\n",
    "    if not batch_first:\n",
    "        src_padding_mask = src_padding_mask.transpose(0, 1)\n",
    "        tgt_padding_mask = tgt_padding_mask.transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "\n",
    "print(look_ahead_mask(6))\n",
    "src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(th.tensor([5,6,7,8,1,1]).view(-1, 1).repeat(1, 2),\n",
    "                                                             th.tensor([1,1,7,8,9,10]).view(-1, 1).repeat(1, 2),\n",
    "                                                             batch_first=False)\n",
    "print(f'src_mask:\\n{src_mask}')\n",
    "print(f'tgt_mask:\\n{tgt_mask}')\n",
    "print(f'src_pad_mask:\\n{src_pad_mask}')\n",
    "print(f'tgt_pad_mask:\\n{tgt_pad_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model: TransformerSeq2Seq,\n",
    "          dataloader: DataLoader,\n",
    "          optimizer: th.optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          device: th.device = th.device('cpu')):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, total=len(dataloader), postfix='Train: ')\n",
    "    for i, (src, tgt) in enumerate(progress_bar):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        decoder_input = tgt[:-1, :] # exclude the last token\n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, decoder_input)\n",
    "        # [seq_len, bs, tgt_vocab_size]\n",
    "        logits = model(src, decoder_input, src_mask, tgt_mask,\n",
    "                       src_padding_mask=src_pad_mask,\n",
    "                       tgt_padding_mask=tgt_pad_mask,\n",
    "                       memory_key_padding_mask=src_pad_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tgt_out = tgt[1:, :].view(-1) # exclude the first token\n",
    "        output = logits.view(-1, logits.shape[-1])\n",
    "        loss = criterion(output, tgt_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {epoch_loss / (i + 1):.3f}')\n",
    "\n",
    "        if i % 30 == 0:\n",
    "            th.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model: TransformerSeq2Seq,\n",
    "             dataloader: DataLoader,\n",
    "             criterion: nn.Module,\n",
    "             device: th.device = th.device('cpu')):\n",
    "\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    progress_bar = tqdm(dataloader, total=len(dataloader), postfix='Eval: ')\n",
    "    with th.no_grad():\n",
    "        for i, (src, tgt) in enumerate(progress_bar):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            decoder_input = tgt[:-1, :] # exclude the last token\n",
    "            src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, decoder_input)\n",
    "            logits = model(src, decoder_input, src_mask, tgt_mask,\n",
    "                           src_padding_mask=src_pad_mask,\n",
    "                           tgt_padding_mask=tgt_pad_mask,\n",
    "                           memory_key_padding_mask=src_pad_mask)\n",
    "\n",
    "            tgt_out = tgt[1:, :].view(-1) # exclude the first token\n",
    "            output = logits.view(-1, logits.shape[-1])\n",
    "            loss = criterion(output, tgt_out)\n",
    "            losses += loss.item()\n",
    "            progress_bar.set_description(f'loss: {losses / (i + 1):.3f}')\n",
    "\n",
    "    th.cuda.empty_cache()\n",
    "    return losses / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model: nn.Module, epochs: int, train_iter: DataLoader, eval_iter: DataLoader,\n",
    "        optimizer: th.optim.Optimizer, criterion: nn.Module,\n",
    "        device: th.device = th.device('cpu')):\n",
    "\n",
    "    history = {'train_loss': [], 'eval_loss': []}\n",
    "    best_eval_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, device)\n",
    "        eval_loss = evaluate(model, eval_iter, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['eval_loss'].append(eval_loss)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Eval Loss: {eval_loss:.3f}')\n",
    "        print('-' * 50)\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            th.save(model.state_dict(), 'best_model.pt')\n",
    "            print('Best model saved!')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(42)\n",
    "\n",
    "src_vocab_size = len(vocab_transform[SRC_LANG])\n",
    "tgt_vocab_size = len(vocab_transform[TGT_LANG])\n",
    "emb_dim = 512\n",
    "n_heads = 8\n",
    "num_enc_layers = 3\n",
    "num_dec_layers = 3\n",
    "dim_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerSeq2Seq(num_enc_layers=num_enc_layers,\n",
    "                           num_dec_layers=num_dec_layers,\n",
    "                           emb_dim=emb_dim,\n",
    "                           n_heads=n_heads,\n",
    "                           src_vocab_size=src_vocab_size,\n",
    "                           tgt_vocab_size=tgt_vocab_size,\n",
    "                           dim_ff=dim_ff,\n",
    "                           dropout=dropout)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# to see the def of Adam, see https://velog.io/@viriditass/%EB%82%B4%EA%B0%80-%EB%B3%B4%EB%A0%A4%EA%B3%A0-%EB%A7%8C%EB%93%A0-Optimizier-%EC%A0%95%EB%A6%AC\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.0001,\n",
    "                          betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_iter = DataLoader(list(zip(train_df[SRC_LANG], train_df[TGT_LANG])),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_iter = DataLoader(list(zip(valid_df[SRC_LANG], valid_df[TGT_LANG])),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_iter = DataLoader(list(zip(test_df[SRC_LANG], test_df[TGT_LANG])),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(th.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.746: 100%|██████████| 6154/6154 [14:29<00:00,  7.08it/s, Train: ]\n",
      "loss: 2.573: 100%|██████████| 770/770 [00:40<00:00, 18.78it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.746 | Eval Loss: 2.573\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.705: 100%|██████████| 6154/6154 [14:20<00:00,  7.15it/s, Train: ]\n",
      "loss: 2.536: 100%|██████████| 770/770 [00:40<00:00, 18.95it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.705 | Eval Loss: 2.536\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.670: 100%|██████████| 6154/6154 [14:18<00:00,  7.17it/s, Train: ]\n",
      "loss: 2.505: 100%|██████████| 770/770 [00:40<00:00, 18.99it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.670 | Eval Loss: 2.505\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.638: 100%|██████████| 6154/6154 [14:18<00:00,  7.17it/s, Train: ]\n",
      "loss: 2.476: 100%|██████████| 770/770 [00:40<00:00, 18.98it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.638 | Eval Loss: 2.476\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.608: 100%|██████████| 6154/6154 [14:24<00:00,  7.12it/s, Train: ]\n",
      "loss: 2.450: 100%|██████████| 770/770 [00:40<00:00, 19.01it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.608 | Eval Loss: 2.450\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.582: 100%|██████████| 6154/6154 [14:23<00:00,  7.13it/s, Train: ]\n",
      "loss: 2.428: 100%|██████████| 770/770 [00:40<00:00, 18.94it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.582 | Eval Loss: 2.428\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.558: 100%|██████████| 6154/6154 [14:20<00:00,  7.15it/s, Train: ]\n",
      "loss: 2.410: 100%|██████████| 770/770 [00:40<00:00, 18.97it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.558 | Eval Loss: 2.410\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "Epoch 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.536: 100%|██████████| 6154/6154 [14:23<00:00,  7.13it/s, Train: ]\n",
      "loss: 2.389: 100%|██████████| 770/770 [00:40<00:00, 18.83it/s, Eval: ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.536 | Eval Loss: 2.389\n",
      "--------------------------------------------------\n",
      "Best model saved!\n",
      "{'train_loss': [2.746309373066493, 2.7051986746095316, 2.669923500616441, 2.63773987841056, 2.608420511248207, 2.581970842581195, 2.5580229495770364, 2.5357390556158785], 'eval_loss': [2.5726006303514755, 2.5362887438241537, 2.50492388520922, 2.4755973481512688, 2.450284817002036, 2.4284961845967676, 2.409927116431199, 2.389207619506043]}\n"
     ]
    }
   ],
   "source": [
    "history = run(model, 8, train_iter, valid_iter, optimizer, criterion, device)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.384: 100%|██████████| 770/770 [00:41<00:00, 18.58it/s, Eval: ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3844373065155824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_history = evaluate(model, test_iter, criterion, device)\n",
    "print(test_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.505:  37%|███▋      | 2282/6154 [05:19<09:02,  7.14it/s, Train: ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(history)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, epochs, train_iter, eval_iter, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, eval_iter, criterion, device)\n\u001b[1;32m     12\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt_out)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = run(model, 4, train_iter, valid_iter, optimizer, criterion, device)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c99887877cf8154d7844f7dd912fc6fe217beebf8c55bc48bb11321ac59cf8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
